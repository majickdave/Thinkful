{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "\n",
    "music = pd.DataFrame()\n",
    "music['duration'] = [184, 134, 243, 186, 122, 197, 294, 382, 102, 264, \n",
    "                     205, 110, 307, 110, 397, 153, 190, 192, 210, 403,\n",
    "                     164, 198, 204, 253, 234, 190, 182, 401, 376, 102]\n",
    "music['loudness'] = [18, 34, 43, 36, 22, 9, 29, 22, 10, 24, \n",
    "                     20, 10, 17, 51, 7, 13, 19, 12, 21, 22,\n",
    "                     16, 18, 4, 23, 34, 19, 14, 11, 37, 42]\n",
    "music['jazz'] = [ 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
    "                  0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
    "                  1, 1, 1, 1, 0, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Tuning KNN\n",
    "\n",
    "While KNN is a relatively simple model, there are several things we can do to tune its performance. These primarily have to do with how we handle distance and how many neighbors we include.\n",
    "\n",
    "## Distance and Normalizing\n",
    "\n",
    "We've talked about the distance measure we use for deciding how close other observations are to a test point, but when we did so we glossed over some important nuance in measuring distance. Specifically, the measurement makes the assumption that all units are equal. So, in our previous example, being 1 loudness unit away is equivalent to being 1 second away. This is intensely problematic and one of the main issues people have with KNN. Units are rarely equivalent, and discerning how to adjust that unequivalence is an abstract and touchy subject. This difficulty also makes binary or categorical variables nearly impossible to include in a KNN model. It really is best if they are continuous. \n",
    "\n",
    "It can be a more obvious challenge if you were dealing with something where the relative scales are strikingly different. For example, if you were looking at buildings and you have height in floors and square footage, you'd have a model that would really only care about square footage since distance in that dimension would be a far greater number of units than the number of floors.\n",
    "\n",
    "To deal with this, typically data scientists will engage in something called __normalization__. Normalization is a way of taking these seemingly incommensurate measures and making them comparable. There are two main normalization techniques that are effective with KNN.\n",
    "\n",
    "1. You can set the bounds of the data to 0 and 1, and then **rescale** every variable to be within those bounds (it may also be reasonable to do -1 to 1, but the difference is actually immaterial). This way every data point is measured in terms of its distance between the max and minimum of its category. This is best if the data shows a linear relationship, such that scaling to a 0 to 1 range makes logical sense. It is also best if there are known limits to the dataset, as those make for logical bounds for 0 and 1 for the rescaling.\n",
    "\n",
    "2. You can also calculate how far each observation is from the mean, expressed in number of standard deviations: this is often called z-scores. Calculating z-scores and using them as your basis for measuring distance works for continuous data and puts everything in terms of how far from the mean (or \"abnormal\") it is.\n",
    "\n",
    "Either of these techniques are viable for most situations and you'll have to use your intuition to see which makes the most sense. Mixing them, while possible, is usually a dangerous proposition.\n",
    "\n",
    "## Weighting\n",
    "\n",
    "There is one more thing to address when talking about distance, and that is weighting. In the vanilla version of KNN, all $k$ of the closest observations are given equal votes on what the outcome of our test observation should be. When the data is densely populated that isn't necessarily a problem. Particularly if there is variance in the measurement itself, not trying to draw information from small differences in distance can be wise.\n",
    "\n",
    "However, sometimes the $k$ nearest observations are not all similarly close to the test. In that case it may be useful to weight by distance. Functionally this will weight by the inverse of distance, so that closer datapoints (with a low distance) have a higher weight than further ones.\n",
    "\n",
    "SKLearn again makes this quite easy to implement. There is an optional weights parameter that can be used when defining the model. Set that parameter to \"distance\" and you will use distance weighting.\n",
    "\n",
    "Let's try it below and see how it affects our model. In this example we'll also use the stats module from SciPy to convert our data to z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFCRJREFUeJzt3X20ZXV93/H3Z2BgYMQgMJZnkWo1kSDQwQZNiBpRSozU\nqis2fdAkzZR22ULTUlC6kpquLB+INDHaZI0PK7RFag1QCZICPlBiUx4G5EEe4lNIMgKCojxpRmb4\n9o/zu+Vw58695965++xz73m/1rrr7rP3vnt/Z885+3P2/u3926kqJEla03cBkqTJYCBIkgADQZLU\nGAiSJMBAkCQ1BoIkCTAQJElNb4GQZF2SG5PcluTOJO/uqxZJEqSvG9OSBFhfVY8nWQt8ETizqq7v\npSBJmnJ79rXiGiTR4+3l2vYzbzrtt/8BteHQw7suTZJWlT+/+45vV9WGhebrLRAAkuwB3Ay8APhw\nVd0w3/wbDj2c37zoyrHUJkmrxS+ccMRfjDJfr43KVbWjqo4DDgdeluSY2fMk2ZRkS5Itj3334fEX\nKUlTYiKuMqqq7wHXAqfOMW1zVW2sqo37PeeAsdcmSdOiz6uMNiTZvw3vA7wGuKeveiRp2vXZhnAI\ncGFrR1gD/I+quqLHeiRpqvV5ldHtwPF9rV+S9EwT0YYgSeqfgSBJAgwESVJjIEiSAANBktQYCJIk\nwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElS\nYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJ\nMBAkSY2BIEkCegyEJEck+UKSu5PcmeTMvmqRJMGePa57O/BvquqWJPsBNye5pqru6rEmSZpavR0h\nVNX9VXVLG34MuBs4rK96JGnaTUQbQpKjgOOBG/qtRJKmV++BkORZwCXAWVX16BzTNyXZkmTLY999\nePwFStKU6DUQkqxlEAYXVdWlc81TVZuramNVbdzvOQeMt0BJmiJ9XmUU4GPA3VV1QV91SJIG+jxC\neAXwj4FXJ7m1/ZzWYz2SNNV6u+y0qr4IpK/1S5KeqfdGZUnSZDAQJEmAgSBJagwESRJgIEiSGgNB\nkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEg\nSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqdmz7wIkSd1Yf/4pi5rfIwRJWoUWGwZgIEjSqrOU\nMAADQZJWlaWGAdiGIEkr3u6EwDCPECRpBVuuMACPECRpxVnOEBhmIEjSCtFVEMwwECRpwnUdBDN6\nDYQkHwdeDzxYVcf0WYskTZJxhcCwvhuV/wA4tecaJGmi9BEG0PMRQlVdl+SoPmuQpEnQVwgMm/g2\nhCSbgE0ABx18WM/VSNLymYQQGDbxgVBVm4HNAEf/2LHVczmStFsmLQSG9d2GIElTY5LDAEY8Qkiy\nAfgV4Kjhv6mqX+qmLElaHSY9BIaNesro08CfAJ8FdizXypNcDLwSOCjJVuDXq+pjy7V8SerDSgqB\nYaMGwr5Vdc5yr7yq/sFyL1OS+rRSwwBGD4QrkpxWVVd2Wo0krUArOQSGjRoIZwLvSvJD4Mk2rqrq\n2d2UJUmTbbWEwLCRAqGq9uu6EElaCVZjEMwY+T6EJG8ATm4vr62qK7opSZImz2oOghmjXnb6XuBE\n4KI26swkP1lV53ZWmST1bBpCYNioRwinAcdV1VMASS4EvgQYCJJWlWkLgWGLuVN5/6HhH1nuQiSp\nb9McBjD6EcJ7gC8l+QIQBm0J7+ysKkkak2kPgWGjXmV0cZJrGbQjBDinqh7osjBJ6oohMLd5AyHJ\ni6vqniQntFFb2+9DkxxaVbd0W54kLR+DYH4LHSH8KoNnEXxgjmkFvHrZK5KkZWYQjGbeQKiqTW3w\n71bVXw9PS7Kus6okaTcZAos3aqPynwInjDBOknplECzdQm0IBwOHAfskOZ5BgzLAs4F9O65NkkZm\nEOy+hY4QXge8HTgcuGBo/GPAuzqqSZJGYggsr4XaEC4ELkzypqq6ZEw1SdK8DIJujHofwiVJfhZ4\nCbBuaPxvdFWYpJVr/fmn8MTZ13SyXHVn1M7tfp9Bm8GrgI8CbwZu7LCuOa351lcn4g3RxRtdWi1m\nPqO7+qwu9vMzCZ/5aTHqVUYvr6pjk9xeVe9O8gHg0i4Lm2RdffuRVrpRdt7D88z3OTIIxm/UQJi5\nB+H7SQ4FvgM8v5uSVoZR39TStFjKDnz23zxx9jUGQY9GDYQ/SrI/cD5wC4O7lD/SWVUrTNdvYANH\nk265PgOGQb8WDIQka4DPVdX3gEuSXAGsq6pHOq9OwNzfoqRJ4A58dVkwEKrqqdZmcFJ7vQ3Y1nVh\n2jVPV2kSGAarz6injK5O8ibg0qqqLgvS4izm6MEjDe0uQ2B1G/WJab8KfArYluTRJI8lebTDurRE\nc31g159/yi7HS6Py/bL6ZSV94X/BgfvUBa87qu8yVozZRwDzfaA9WtBcDIHV4fSL77m5qjYuNN+o\nN6adPNf4qrpusYVpfGbfLzE8PPuD7r0VA4amITDNRm1DOHtoeB3wMuBmfEDOijVXOMz8npYd34xR\nd4BzzbeatpVBoFH7Mvq54ddJjgDe30lFGruZndpwMKymHd2uLMcOcDWEqEGgGaMeIcy2FThmOQtR\nNxazc1/JO7XF6GIHuJIuBTYAtCujtiH8LoO7k2FwZdJxwG1dFaXlNS3f+Oczzp3gJG5vQ0CjGPUI\nYcvQ8Hbg4qr6Px3Uo45M4k5qHPraEU7C9jYEtFijtiFcmGRDG36o25LUldVwvnsUk7IjHOf2npR/\ns1a2hZ6pHODXgXcweJ7ymiTbgd9djofjJDkV+B1gD+CjVfXe3V2mFrZag2FSd4o+LEYrxUJHCGcB\nrwBOrKo/B0hyNPB7Sf51Vf2npa44yR7Ah4FTGDRS35Tk8qq6a6nL1OJMwmmNpVjJO8KlND6v5H+v\nVpZ571RO8iXglKr69qzxG4Crq+r4Ja84OQn4D1X1uvb6nQBV9Z5d/Y13KndnJQSDO0ZpaZbrTuW1\ns8MABu0ISdYuubqBw4C/Gnq9Ffg7u7lMrUIGgTQeCwXCD5c4bRSZY9xOhytJNgGbADbsu9TbJrSQ\nlXQdvaRuLNTb6Utb76azfx4Dfnw3170VOGLo9eHAfbNnqqrNVbWxqjY+e52BMA5+I5em07x72Kra\no8N13wS8MMnzgW8CbwV+ocP1SZLm0dtX7qranuQdwFUMLjv9eFXd2Vc9eqZJOoW07bVnsebeW9jz\nK39Cdj6rKGmZjPqAnE5U1ZVV9beq6m9W1W/2WYt2re9TSNtf+rM8+ap/Bnss/vtL32EmrSS9BoJW\njr5DoZ79XJ488S3UnnuPfIwwEwaGgjQaW2k1sr7vcH7yp36RHUcex9rbr4Qnt7H9R1/Jjhf9NKzp\nsqlLmh4Gghatzzucn3re8Wx73uLvh3zi7Gt6P8qRJp2njLQk688/ZcXtYD11JM3PQNBuWYmhYDBI\nczMQtNvmDIUq8uiD8MR3R1tIFXnkAfj+95a3OEkjsw1By2K4wXnNN+9k78+8lzzxMFSx7bRz2PGi\nkyFz9VYCa/7yVva+8v3kB49CPcVTh7yYbT93HvWsAzur1zYFaWceIWhZrT//FPb5xFmseeQBsv2H\nZMeTrL36t4GCuXrW/d4DrLvk37PmsYfI9m1kx5Os+eZdrPvkv5t7/mXkqSPpmQwEdW6PbY+z16W/\nBvXUTtPW3no52b7tGeNSO1jz8F+y/rde2/m3eENBepqBoLFY+40bWP+BU58xbv35p7DXTZ9a8G8N\nBWk8DASN1czlqovdyRsKUvcMBKkxFDTtDARpiKGgaWYgSJIAA0ErxDi/uXs3s6aVgaCJ19fO2VDQ\ntDEQNPGWemXScjAUNE3sukIryuxQcIctLR8DQSvaKEcNc4WG/RhJOzMQtOq585dGYxuCJAkwECRJ\njYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktT0EghJ\n3pLkziRPJdnYRw2SpGfq6wjhy8DfB67raf2SpFl6eUBOVd0NkKSP1UuS5jDxbQhJNiXZkmTLo3+9\nve9yJGnV6uwIIclngYPnmHReVX161OVU1WZgM8ALDtynlqk8SdIsnQVCVb2mq2VLkpbfxJ8ykiSN\nR1+Xnb4xyVbgJOAzSa7qow5J0tP6usroMuCyPtYtSZqbp4wkSYCBIElqDARJEmAgSJIaA0GSBBgI\nkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwE\nSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaC\nJKkxECRJQE+BkOT8JPckuT3JZUn276MOSdLT+jpCuAY4pqqOBb4CvLOnOiRJTS+BUFVXV9X29vJ6\n4PA+6pAkPW0S2hB+CfjjvouQpGm3Z1cLTvJZ4OA5Jp1XVZ9u85wHbAcummc5m4BNABv27axcSZp6\nne1hq+o1801P8jbg9cDPVFXNs5zNwGaAFxy4zy7nkyTtnl6+cic5FTgH+Omq+n4fNUiSninzfDnv\nbqXJ14C9ge+0UddX1Rkj/N1DwF90WRtwEPDtjtexVNa2NNa2NNa2NJNY2/OqasNCM/USCJMsyZaq\n2th3HXOxtqWxtqWxtqWZ5NoWMglXGUmSJoCBIEkCDIS5bO67gHlY29JY29JY29JMcm3zsg1BkgR4\nhCBJaqY+EJK8JcmdSZ5KsssrA5Lcm+SOJLcm2TJhtZ2a5M+SfC3JuWOq7YAk1yT5avv9nF3Mt6Nt\ns1uTXN5xTfNuhyR7J/lkm35DkqO6rGeRtb09yUND2+qfjqmujyd5MMmXdzE9ST7Y6r49yQnjqGvE\n2l6Z5JGhbfZrY6ztiCRfSHJ3+4yeOcc8vW27Jauqqf4BfhR4EXAtsHGe+e4FDpq02oA9gK8DRwN7\nAbcBPzaG2t4PnNuGzwXet4v5Hh/TtlpwOwD/Avj9NvxW4JMTVNvbgQ+N8/3V1nsycALw5V1MP41B\nX2MBfgK4YYJqeyVwxbi3WVv3IcAJbXg/Br02z/4/7W3bLfVn6o8QquruqvqzvuuYy4i1vQz4WlV9\no6p+CPx34PTuq+N04MI2fCHw98awzvmMsh2Ga/5D4GeSZEJq60VVXQc8PM8spwP/pQauB/ZPcsiE\n1Nabqrq/qm5pw48BdwOHzZqtt223VFMfCItQwNVJbm4d7k2Kw4C/Gnq9lZ3fmF34G1V1Pww+HMBz\ndzHfuiRbklyfpMvQGGU7/P95atD9+iPAgR3WtJjaAN7UTi38YZIjxlDXKPp6f43qpCS3JfnjJC/p\no4B26vF44IZZkyZ92+1kKroPHaXn1RG8oqruS/Jc4Jok97RvMH3XNtc33GW5dGy+2haxmCPbdjsa\n+HySO6rq68tR3yyjbIfOttUCRlnvHwEXV9W2JGcwOJJ5deeVLayvbTaKWxh0yfB4ktOA/wm8cJwF\nJHkWcAlwVlU9OnvyHH8yKdtuTlMRCLVAz6sjLuO+9vvBJJcxOA2w24GwDLVtBYa/TR4O3LebywTm\nry3Jt5IcUlX3t8PgB3exjJnt9o0k1zL4JtVFIIyyHWbm2ZpkT+BHGM8piQVrq6rvDL38CPC+MdQ1\nis7eX7treAdcVVcm+c9JDqqqsfQjlGQtgzC4qKounWOWid12u+IpoxEkWZ9kv5lh4LXAnFc+9OAm\n4IVJnp9kLwaNpZ1ezdNcDrytDb8N2OloJslzkuzdhg8CXgHc1VE9o2yH4ZrfDHy+Wutfxxasbda5\n5TcwOCc9CS4H/km7YuYngEdmThX2LcnBM21ASV7GYH/2nfn/atnWHeBjwN1VdcEuZpvYbbdLfbdq\n9/0DvJFBkm8DvgVc1cYfClzZho9mcGXIbcCdDE7nTERt7fVpDK5y+PoYazsQ+Bzw1fb7gDZ+I/DR\nNvxy4I623e4AfrnjmnbaDsBvAG9ow+uATwFfA24Ejh7j+2yh2t7T3lu3AV8AXjymui4G7geebO+1\nXwbOAM5o0wN8uNV9B/NciddDbe8Y2mbXAy8fY20/yeD0z+3Are3ntEnZdkv98U5lSRLgKSNJUmMg\nSJIAA0GS1BgIkiTAQJAkNQaCpkqSxztY5h8kefNyL1caNwNBkgQYCBJJnpfkc61juc8lObKNf8Y3\n/5mji3bn6YeS3JXkMwx17JfBczPeneSWDJ6f8eI2fn3r3/+mJF9Kcnob/5IkN7b+/G9P8sI272da\np21fTvLzY90gmloGggQfYtBN8bHARcAHF5j/jQyeU/HjwK8wuCN72Ler6gTg94B/28adx6CrjBOB\nVwHnt25QzgB+p6qOY3CX91bgVOC+qnppVR0D/K/d/QdKozAQJDgJ+EQb/q8MuiWYz8kMeibdUYPO\n+z4/a/pMR2c3A0e14dcC5ya5lcEDj9YBRwL/F3hXknMY9Nz5AwbdHLwmyfuS/FRVPbLkf5m0CAaC\ntLOZ/ly20z4jrTOzveaYZy7b2u8dPN2jcIA3VdVx7efIGjwA6RMMOrP7AXBVkldX1VeAv80gGN4z\nzkdDaroZCBL8KYMeSAH+IfDFNnwvgx0zDJ5+tbYNXwe8NckerZfSV42wjquAfznUO+fx7ffRwDeq\n6oMMesc8NsmhwPer6r8Bv8XgMZJS56bieQjSkH2TbB16fQHwr4CPJzkbeAj4xTbtI8Cnk9zIoEfX\nJ9r4yxg8vOYOBj2Y/u8R1vsfgd8Gbm+hcC/weuDngX+U5EngAQY9oJ7IoI3hKQY9ff7zpf1TpcWx\nt1NJEuApI0lSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAuD/AXrlNrjitYmSAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67f35442e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import stats\n",
    "\n",
    "neighbors = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "\n",
    "# Our input data frame will be the z-scores this time instead of raw data.\n",
    "X = pd.DataFrame({\n",
    "    'loudness': stats.zscore(music.loudness),\n",
    "    'duration': stats.zscore(music.duration)\n",
    "})\n",
    "\n",
    "# Fit our model.\n",
    "Y = music.jazz\n",
    "neighbors.fit(X, Y)\n",
    "\n",
    "# Arrays, not data frames, for the mesh.\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Mesh size.\n",
    "h = .01\n",
    "\n",
    "# Plot the decision boundary. We assign a color to each point in the mesh.\n",
    "x_min = X[:,0].min() - .5\n",
    "x_max = X[:,0].max() + .5\n",
    "y_min = X[:,1].min() - .5\n",
    "y_max = X[:,1].max() + .5\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, h),\n",
    "    np.arange(y_min, y_max, h)\n",
    ")\n",
    "Z = neighbors.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.set_cmap(plt.cm.Paired)\n",
    "plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "# Add the training points to the plot.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.xlabel('Loudness')\n",
    "plt.ylabel('Duration')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "This is a much more nuanced decision boundary, but it's also relatively continuous and consistent, providing a nice sense of which regions are likely to be which classification.\n",
    "\n",
    "\n",
    "## Choosing K\n",
    "\n",
    "The last major aspect of tuning KNN is picking $k$. This choice is largely up to the data scientist building the model but there are some things to consider.\n",
    "\n",
    "Choosing $k$ is a tradeoff. The larger the $k$ the more smoothed out your decision space will be, with more observations getting a vote in the prediction. A smaller $k$ will pick up more subtle deviations, but these deviations could be just randomness and therefore you could just be overfitting. Add in weighting and that's an additional dimension to this entire conversation.\n",
    "\n",
    "In the end, the best technique is probably to try multiple models and use your validation techniques to see which is best. In particular, k-fold cross validation is a great way to see how your KNN model is performing.\n",
    "\n",
    "\n",
    "## DRILL:\n",
    "\n",
    "Let's say we work at a credit card company and we're trying to figure out if people are going to pay their bills on time. We have everyone's purchases, split into four main categories: groceries, dining out, utilities, and entertainment. What are some ways you might use KNN to create this model? What aspects of KNN would be useful? Write up your thoughts in submit a link below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Based on the data provided, the two different ways a KNN classifier could be a very successful model are by measuring both **groceries vs. dining out** and **utilities vs. entertainment**. This is assuming that there is a variable indicating whether or not bills were paid on time or not. I think choosing a \"medium\" k value would also give us a good balance between smoothness and picking up small deviations."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "105px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
