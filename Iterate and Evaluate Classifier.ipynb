{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "Will be using a Naive Bayes Classifier on Yelp review data, in order to determine if feedback was 'positive' or 'negative'. This will require feature engineering in order to produce the most accurate classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/str/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearngit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e07622806b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearngit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearngit'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearngit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read text file in and assign own headers\n",
    "\n",
    "yelp_raw = pd.read_csv('yelp_labelled.txt', delimiter= '\\t', header=None)\n",
    "yelp_raw.columns = ['Review', 'Positive or Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the data\n",
    "\n",
    "yelp_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create keywords list to form the basis of our Naive Bayes Classifier\n",
    "\n",
    "keywords = ['worst', 'slow', 'pricey', 'poor', 'bad', 'rude', 'disappointed', \n",
    "            'avoid', 'nasty', 'sick', 'average', 'not', 'terrible', 'wait', 'but',\n",
    "            'good', 'like', 'great', 'definitely', 'don\\'t', 'recommend', 'didn\\'t']\n",
    "\n",
    "for key in keywords:\n",
    "    yelp_raw[str(key)] = yelp_raw.Review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Since one of the main assumptions of the Naive Bayes Classifier is that the variables are independent of eachother, let's look at a correlation matrix to see if this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix with sns.heatmap\n",
    "\n",
    "# Set up the matplotlib figure.\n",
    "f, ax = plt.subplots(figsize=(18, 15))\n",
    "\n",
    "sns.heatmap(yelp_raw.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix should be good enough. Ideally, we would want no correlation whatsoever. However, although there is some correlation across the board, there is none greater than 0.25 and that should still yield strong results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we actually run the model we have to build out our training data. Specify an outcome (y or dependent variable) and \n",
    "#the inputs (x or independent variables). We'll do that below under the variables data and target\n",
    "\n",
    "data = yelp_raw[keywords]\n",
    "target = yelp_raw['Positive or Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since data is binary / boolean, need to import the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "NB_Model = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "NB_Model.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "positive_predictor = NB_Model.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != positive_predictor).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix to better understand results\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, positive_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at our first model and then create a few more versions.\n",
    "1. Do any of your classifiers seem to overfit?\n",
    "2. Which seem to perform the best? Why?\n",
    "3. Which features seemed to be most impactful to performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New column that splits reviews\n",
    "yelp_raw['ReviewSplit'] = yelp_raw['Review'].apply(lambda x: str(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get a unique count of each word in all reviews. First create 'counts' variable.\n",
    "\n",
    "from collections import Counter\n",
    "counts = yelp_raw.ReviewSplit.map(Counter).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View most commmon words\n",
    "\n",
    "counts.most_common(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1** - Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(NB_Model, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, positive_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** Scores are not fluctuating too much during cross-validation, indicating that the model is not overfitting. Further, the confusion matrix shows that the model is far more accurate in predicting negative reviews, specifically at a 96.2% rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2** - Remove a Couple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at 'Sick' and 'Nasty' to see how common they are\n",
    "\n",
    "print(counts['sick'])\n",
    "print(counts['nasty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since these are not common at all, let's remove them and run our model\n",
    "\n",
    "yelp_raw.drop(columns=['sick', 'nasty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish Y and X variables\n",
    "data2 = yelp_raw.loc[:,'worst':'wait']\n",
    "target2 = yelp_raw['Positive or Negative']\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "NB_Model2 = BernoulliNB()\n",
    "\n",
    "\n",
    "# Fit our model to the data.\n",
    "NB_Model2.fit(data2, target2)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "positive_predictor2 = NB_Model2.predict(data2)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data2.shape[0],\n",
    "    (target2 != positive_predictor2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(NB_Model2, data2, target2, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target2, positive_predictor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** After removing 'sick' and 'nasty' from the original model, there was not much change in the results. Scores are not fluctuating too much during cross-validation, indicating that the model is not overfitting. However, in the confusion matrix, you can see that while our prediction of negative reviews remained the same, our prediction of positive reviews dropped from 27.4% to 26%. Not a large difference, but a difference nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 3** - Add a couple new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_model3 = pd.read_csv('yelp_labelled.txt', delimiter= '\\t', header=None)\n",
    "yelp_model3.columns = ['Review', 'Positive or Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After reviewing the most common counts above, decided to add a few new features ('but', 'won't')\n",
    "\n",
    "keywords = ['worst', 'slow', 'pricey', 'poor', 'bad', 'rude', 'disappointed', \n",
    "            'avoid', 'nasty', 'sick', 'average', 'not', 'terrible', 'wait', 'but', 'won\\'t']\n",
    "\n",
    "for key in keywords:\n",
    "    yelp_model3[str(key)] = yelp_model3.Review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish Y and X variables\n",
    "data3 = yelp_model3[keywords]\n",
    "target3 = yelp_model3['Positive or Negative']\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "NB_Model3 = BernoulliNB()\n",
    "\n",
    "\n",
    "# Fit our model to the data.\n",
    "NB_Model3.fit(data3, target3)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "positive_predictor3 = NB_Model3.predict(data3)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data3.shape[0],\n",
    "    (target3 != positive_predictor3).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(NB_Model3, data3, target3, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target3, positive_predictor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** Seems that there may be some overfitting based on the fluctuation seen during cross-validation. However, the model was more successful by adding 'but' and 'won't to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 4** - Add a couple new features and remove a couple existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at a couple existing features to see how prevalent they are\n",
    "\n",
    "print(counts['pricey'])\n",
    "print(counts['rude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_model4 = pd.read_csv('yelp_labelled.txt', delimiter= '\\t', header=None)\n",
    "yelp_model4.columns = ['Review', 'Positive or Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 'don't' and 'never' to the model, remove 'pricey' and 'rude'\n",
    "\n",
    "keywords = ['worst', 'slow', 'poor', 'bad', 'disappointed', 'avoid', 'nasty', 'sick',\n",
    "            'average', 'not', 'terrible', 'wait', 'but', 'don\\'t', 'never']\n",
    "\n",
    "for key in keywords:\n",
    "    yelp_model4[str(key)] = yelp_model4.Review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish Y and X variables\n",
    "data4 = yelp_model4[keywords]\n",
    "target4 = yelp_model4['Positive or Negative']\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "NB_Model4 = BernoulliNB()\n",
    "\n",
    "\n",
    "# Fit our model to the data.\n",
    "NB_Model4.fit(data4, target4)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "positive_predictor4 = NB_Model4.predict(data4)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data4.shape[0],\n",
    "    (target4 != positive_predictor4).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(NB_Model4, data4, target4, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target4, positive_predictor4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** Similar to the last model version, we are seeing evidence of overfitting once again. At the same time, our model is the most successful it has been thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 5** - Change features to try and identify positive reviews instead of negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_model5 = pd.read_csv('yelp_labelled.txt', delimiter= '\\t', header=None)\n",
    "yelp_model5.columns = ['Review', 'Positive or Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redo model using positive words\n",
    "\n",
    "keywords = ['Good', 'good', 'like', 'great', 'best', 'recommend', 'definitely', 'love',\n",
    "            'nice', 'good.', 'really', 'quality', 'back.']\n",
    "\n",
    "for key in keywords:\n",
    "    yelp_model5[str(key)] = yelp_model5.Review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish Y and X variables\n",
    "data5 = yelp_model5[keywords]\n",
    "target5 = yelp_model5['Positive or Negative']\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "NB_Model5 = BernoulliNB()\n",
    "\n",
    "\n",
    "# Fit our model to the data.\n",
    "NB_Model5.fit(data5, target5)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "positive_predictor5 = NB_Model5.predict(data5)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data5.shape[0],\n",
    "    (target5 != positive_predictor5).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(NB_Model5, data5, target5, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target5, positive_predictor5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** Evidence for overfitting not as strong in previous model versions. However, we did lose overall accuracy. We did accomplish the goal of this version by seeing a very large reduction in Type 1 errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function for backwards pass - removing features and running model\n",
    "\n",
    "while len(data.columns) > 1:\n",
    "    data.drop(data.columns[[-1,]], axis=1, inplace=True)\n",
    "    NB_Model = BernoulliNB()\n",
    "    NB_Model.fit(data, target)\n",
    "    positive_predictor = NB_Model.predict(data)\n",
    "    print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != positive_predictor).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:** Based on the output above, looking at our original model, it looks like most of our features are marginally important. The most important by far is the keyword 'not', which corresponds in the change from 385 to 451 mislabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_model6 = pd.read_csv('yelp_labelled.txt', delimiter= '\\t', header=None)\n",
    "yelp_model6.columns = ['Review', 'Positive or Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Good', 'good', 'like', 'great', 'best', 'recommend', 'definitely', 'love',\n",
    "            'nice', 'good.', 'really', 'quality', 'back.']\n",
    "\n",
    "for key in keywords:\n",
    "    yelp_model6[str(key)] = yelp_model6.Review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Establish Y and X variables\n",
    "data6 = yelp_model6[keywords]\n",
    "target6 = yelp_model6['Positive or Negative']\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "NB_Model6 = GaussianNB()\n",
    "\n",
    "\n",
    "# Fit our model to the data.\n",
    "NB_Model6.fit(data6, target6)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "positive_predictor6 = NB_Model6.predict(data6)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data6.shape[0],\n",
    "    (target6 != positive_predictor6).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
