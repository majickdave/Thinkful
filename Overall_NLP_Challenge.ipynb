{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "I will be using two Chesterton texts, Father Brown and The Man Who Was Thursday, along with two Austen texts, Emma and Persuasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "#Search Gutenberg Corpus and find texts\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning, Processing and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the raw text\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Inspect raw files first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n",
      "\n",
      "\n",
      " END\n",
      "[Persuasion by Jane Austen 1818]\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Sir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who,\n",
      "for his own amusement, never took up any book but the Baronetage;\n",
      "there he found occupation for an idle hour, and consolation in a\n",
      "distressed one; there his faculties were roused into admiration and\n",
      "respect, by contemplating the limited remnant of the earliest patents;\n",
      "there any unwelcome sensations, arising from domestic affairs\n",
      "changed naturally into pity and contempt as he turn\n",
      "\n",
      "\n",
      " END\n",
      "[The Wisdom of Father Brown by G. K. Chesterton 1914]\n",
      "\n",
      "\n",
      "I. The Absence of Mr Glass\n",
      "\n",
      "\n",
      "THE consulting-rooms of Dr Orion Hood, the eminent criminologist\n",
      "and specialist in certain moral disorders, lay along the sea-front\n",
      "at Scarborough, in a series of very large and well-lighted french windows,\n",
      "which showed the North Sea like one endless outer wall of blue-green marble.\n",
      "In such a place the sea had something of the monotony of a blue-green dado:\n",
      "for the chambers themselves were ruled throughout by a \n",
      "\n",
      "\n",
      " END\n",
      "[The Man Who Was Thursday by G. K. Chesterton 1908]\n",
      "\n",
      "To Edmund Clerihew Bentley\n",
      "\n",
      "A cloud was on the mind of men, and wailing went the weather,\n",
      "Yea, a sick cloud upon the soul when we were boys together.\n",
      "Science announced nonentity and art admired decay;\n",
      "The world was old and ended: but you and I were gay;\n",
      "Round us in antic order their crippled vices came--\n",
      "Lust that had lost its laughter, fear that had lost its shame.\n",
      "Like the white lock of Whistler, that lit our aimless gloom,\n",
      "Men showed their \n"
     ]
    }
   ],
   "source": [
    "#Print first 500 characters of each, followed by 'END'\n",
    "print(emma[0:500])\n",
    "print('\\n\\n END')\n",
    "\n",
    "print(persuasion[0:500])\n",
    "print('\\n\\n END')\n",
    "\n",
    "print(brown[0:500])\n",
    "print('\\n\\n END')\n",
    "\n",
    "print(thursday[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Clean and Process the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    text = re.sub(r'Chapter \\d+', '', text)\n",
    "    text = re.sub(r'VOLUME \\w+', '', text)\n",
    "    text = re.sub(r'CHAPTER \\w+', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text cleaning utility on our texts\n",
    "emma = text_cleaner(emma)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "brown = text_cleaner(brown)\n",
    "thursday = text_cleaner(thursday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Parse the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned texts\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "emma_doc = nlp(emma)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "brown_doc = nlp(brown)\n",
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8905\n",
      "3656\n",
      "3716\n",
      "3490\n"
     ]
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "brown_sents = [[sent, \"Chesterton\"] for sent in brown_doc.sents]\n",
    "thursday_sents = [[sent, \"Chesterton\"] for sent in thursday_doc.sents]\n",
    "\n",
    "print(len(emma_sents))\n",
    "print(len(persuasion_sents))\n",
    "print(len(brown_sents))\n",
    "print(len(thursday_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#For computational purposes, reduce length of each set of sentences to 500\n",
    "emma_sents = emma_sents[0:500]\n",
    "persuasion_sents = persuasion_sents[0:500]\n",
    "brown_sents = brown_sents[0:500]\n",
    "thursday_sents = thursday_sents[0:500]\n",
    "\n",
    "print(len(emma_sents))\n",
    "print(len(persuasion_sents))\n",
    "print(len(brown_sents))\n",
    "print(len(thursday_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...  Austen\n",
       "1  (She, was, the, youngest, of, the, two, daught...  Austen\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  Austen\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  Austen\n",
       "4                                 (Between, _, them)  Austen"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Austen' 'Chesterton']\n"
     ]
    }
   ],
   "source": [
    "# Combine the sentences from the four texts into one data frame.\n",
    "sentences = pd.DataFrame(emma_sents + persuasion_sents + brown_sents + thursday_sents)\n",
    "\n",
    "#Confirm this worked\n",
    "display(sentences.head(5))\n",
    "\n",
    "#Confirm we only have two authors across four texts\n",
    "print(sentences.iloc[:, 1].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 1000 most common words\n",
    "\n",
    "def bag_of_words(text):\n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "        \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Create column headers for sentence text and source (author) and initialize to 0\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags for each text\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(emmawords + persuasionwords + brownwords + thursdaywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assistance</th>\n",
       "      <th>somewhat</th>\n",
       "      <th>favourite</th>\n",
       "      <th>thorough</th>\n",
       "      <th>goddard</th>\n",
       "      <th>pleasing</th>\n",
       "      <th>steam</th>\n",
       "      <th>series</th>\n",
       "      <th>shirley</th>\n",
       "      <th>conversation</th>\n",
       "      <th>...</th>\n",
       "      <th>attachment</th>\n",
       "      <th>acknowledge</th>\n",
       "      <th>convey</th>\n",
       "      <th>heaven</th>\n",
       "      <th>walter</th>\n",
       "      <th>instead</th>\n",
       "      <th>daylight</th>\n",
       "      <th>tenderness</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Between, _, them)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(_)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(it, was, more, the, intimacy, of, sisters, .)</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Even, before, Miss, Taylor, had, ceased, to, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, real, evils, ,, indeed, ,, of, Emma, 's,...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, danger, ,, however, ,, was, at, present,...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2035 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  assistance somewhat favourite thorough goddard pleasing steam series  \\\n",
       "0          0        0         0        0       0        0     0      0   \n",
       "1          0        0         0        0       0        0     0      0   \n",
       "2          0        0         0        0       0        0     0      0   \n",
       "3          0        0         0        0       0        0     0      0   \n",
       "4          0        0         0        0       0        0     0      0   \n",
       "5          0        0         0        0       0        0     0      0   \n",
       "6          0        0         0        0       0        0     0      0   \n",
       "7          0        0         0        0       0        0     0      0   \n",
       "8          0        0         0        0       0        0     0      0   \n",
       "9          0        0         0        0       0        0     0      0   \n",
       "\n",
       "  shirley conversation     ...     attachment acknowledge convey heaven  \\\n",
       "0       0            0     ...              0           0      0      0   \n",
       "1       0            0     ...              0           0      0      0   \n",
       "2       0            0     ...              0           0      0      0   \n",
       "3       0            0     ...              0           0      0      0   \n",
       "4       0            0     ...              0           0      0      0   \n",
       "5       0            0     ...              0           0      0      0   \n",
       "6       0            0     ...              0           0      0      0   \n",
       "7       0            0     ...              0           0      0      0   \n",
       "8       0            0     ...              0           0      0      0   \n",
       "9       0            0     ...              0           0      0      0   \n",
       "\n",
       "  walter instead daylight tenderness  \\\n",
       "0      0       0        0          0   \n",
       "1      0       0        0          0   \n",
       "2      0       0        0          0   \n",
       "3      0       0        0          0   \n",
       "4      0       0        0          0   \n",
       "5      0       0        0          0   \n",
       "6      0       0        0          0   \n",
       "7      0       0        0          0   \n",
       "8      0       0        0          0   \n",
       "9      0       0        0          0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...      Austen  \n",
       "1  (She, was, the, youngest, of, the, two, daught...      Austen  \n",
       "2  (Her, mother, had, died, too, long, ago, for, ...      Austen  \n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...      Austen  \n",
       "4                                 (Between, _, them)      Austen  \n",
       "5                                                (_)      Austen  \n",
       "6     (it, was, more, the, intimacy, of, sisters, .)      Austen  \n",
       "7  (Even, before, Miss, Taylor, had, ceased, to, ...      Austen  \n",
       "8  (The, real, evils, ,, indeed, ,, of, Emma, 's,...      Austen  \n",
       "9  (The, danger, ,, however, ,, was, at, present,...      Austen  \n",
       "\n",
       "[10 rows x 2035 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of Speech Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting Parts of Speech\n",
    "\n",
    "def get_pos(df):\n",
    "    all_pos= []\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        for token in sentence:\n",
    "            if token.pos != 0:\n",
    "                all_pos.append(token.pos_)\n",
    "    return all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create unique column for each entity in words_count\n",
    "\n",
    "pos_types = set(get_pos(word_counts))\n",
    "pos = pd.DataFrame(columns=pos_types)\n",
    "word_counts = pd.concat([word_counts, pos], axis=1)\n",
    "word_counts.loc[:, pos_types] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, produce counts for each entity by sentence\n",
    "\n",
    "for i, sentence in enumerate(word_counts['text_sentence']):\n",
    "    for token in sentence:\n",
    "        if token.pos != 0:\n",
    "            word_counts.loc[i, token.pos_] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entities Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting Entities\n",
    "\n",
    "def get_entity_types(df):\n",
    "    all_entities= []\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        for token in sentence:\n",
    "            if token.ent_type != 0:\n",
    "                all_entities.append(token.ent_type_)\n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create unique column for each entity in words_count\n",
    "\n",
    "entity_types = set(get_entity_types(word_counts))\n",
    "entities = pd.DataFrame(columns=entity_types)\n",
    "word_counts = pd.concat([word_counts, entities], axis=1)\n",
    "word_counts.loc[:, entity_types] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, produce counts for each entity by sentence\n",
    "\n",
    "for i, sentence in enumerate(word_counts['text_sentence']):\n",
    "    for token in sentence:\n",
    "        if token.ent_type != 0:\n",
    "            word_counts.loc[i, token.ent_type_] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of words in sentence\n",
    "\n",
    "word_counts['word_count'] = 0\n",
    "\n",
    "for i, sentence in enumerate(word_counts['text_sentence']):\n",
    "    words = len([token for token in sentence if not token.is_punct])\n",
    "        \n",
    "    # Populate the row with word counts.\n",
    "    word_counts.loc[i, 'word_count'] += words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models with BoW Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austen        1000\n",
       "Chesterton    1000\n",
       "Name: text_source, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Baseline of our dataset is 50%\n"
     ]
    }
   ],
   "source": [
    "#Determine Baseline\n",
    "display(word_counts.text_source.value_counts())\n",
    "print('\\n\\nBaseline of our dataset is 50%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, establish X and Y\n",
    "\n",
    "Y = word_counts['text_source']\n",
    "X = word_counts.drop(['text_sentence','text_source'], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Regular Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9973333333333333\n",
      "\n",
      "Test set score: 0.788\n",
      "\n",
      "Cross-Validation: [0.7625 0.77   0.765  0.75   0.76  ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr = LogisticRegression(penalty='l2', C=1e9)\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lasso Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.918\n",
      "\n",
      "Test set score: 0.842\n",
      "\n",
      "Cross-Validation: [0.7575 0.7975 0.7975 0.8025 0.7925]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr_lasso = LogisticRegression(penalty='l1')\n",
    "train = lr_lasso.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr_lasso.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr_lasso.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr_lasso, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.972\n",
      "\n",
      "Test set score: 0.868\n",
      "\n",
      "Cross-Validation: [0.81   0.8425 0.8125 0.795  0.81  ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "lr_ridge = LogisticRegression(penalty='l2')\n",
    "train = lr_ridge.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', lr_ridge.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr_ridge.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(lr_ridge, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6426666666666667\n",
      "\n",
      "Test set score: 0.642\n",
      "\n",
      "Cross-Validation: [0.53   0.6375 0.64   0.63   0.6775]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "svc = SVC()\n",
    "train = svc.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(svc, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9866666666666667\n",
      "\n",
      "Test set score: 0.766\n",
      "\n",
      "Cross-Validation: [0.685  0.765  0.7625 0.715  0.745 ]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate and fit\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "#Scoring\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "cross_val = cross_val_score(rfc, X, Y, cv=5)\n",
    "print('\\nCross-Validation:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame with TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "#Create vectorizer model in order to get tf-idf for each sentence\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=False, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Take pandas series (text_sentence), convert from spacy object to string\n",
    "sentence_list = word_counts['text_sentence'].astype(str)\n",
    "print(type(sentence_list))\n",
    "\n",
    "#Pass pandas series to our vectorizer model\n",
    "text_tfidf = vectorizer.fit_transform(sentence_list)\n",
    "print(type(text_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Emma': 0.1868627847541369,\n",
       " 'Woodhouse': 0.20815540640123767,\n",
       " 'best': 0.23553762621408428,\n",
       " 'blessings': 0.2850270159696958,\n",
       " 'clever': 0.24882969029871252,\n",
       " 'comfortable': 0.24882969029871252,\n",
       " 'disposition': 0.2690094441926767,\n",
       " 'distress': 0.27621191011155916,\n",
       " 'existence': 0.2850270159696958,\n",
       " 'handsome': 0.2214474704858659,\n",
       " 'happy': 0.23026257634400263,\n",
       " 'home': 0.21768231877350422,\n",
       " 'little': 0.16031226287591815,\n",
       " 'lived': 0.22560965260281096,\n",
       " 'nearly': 0.26291984602693086,\n",
       " 'rich': 0.23553762621408428,\n",
       " 'world': 0.20542989870884681,\n",
       " 'years': 0.1960915508603913}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#Shape\n",
    "n = text_tfidf.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bysent = [{} for _ in range(0,n)]\n",
    "\n",
    "#for each sentence, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*text_tfidf.nonzero()):\n",
    "    tfidf_bysent[i][terms[j]] = text_tfidf[i, j]\n",
    "\n",
    "#Show first dictionary\n",
    "display(tfidf_bysent[0])\n",
    "print(type(tfidf_bysent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe for this feature set\n",
    "tfidf_df = pd.DataFrame(columns=terms)\n",
    "tfidf_df['text_sentence'] = word_counts['text_sentence']\n",
    "tfidf_df['text_source'] = word_counts['text_source']\n",
    "tfidf_df.loc[:, terms] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Emma' 'Woodhouse' 'handsome' 'clever' 'rich' 'comfortable' 'home'\\n 'happy' 'disposition' 'best' 'blessings' 'existence' 'lived' 'nearly'\\n 'years' 'world' 'little' 'distress'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-210e3f96656c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfidf_bysent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtfidf_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_tuple\u001b[0;34m(self, key, is_setter)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Too many indexers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_setter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0mkeyidx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1269\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Emma' 'Woodhouse' 'handsome' 'clever' 'rich' 'comfortable' 'home'\\n 'happy' 'disposition' 'best' 'blessings' 'existence' 'lived' 'nearly'\\n 'years' 'world' 'little' 'distress'] not in index\""
     ]
    }
   ],
   "source": [
    "for i in tfidf_bysent:\n",
    "    for k, v in i.items():\n",
    "        tfidf_df.loc[i, k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
